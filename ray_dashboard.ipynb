{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cceaad-8ebe-4e20-bfd9-d4a48d39ae71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cdsw\n",
    "import ray\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b2bcc-a456-48b3-b648-470bdda6f2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ray start --head --block --include-dashboard=true --dashboard-port=8100 --num-gpus=1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216899ba-8066-477b-bc71-3edd7a32064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DASHBOARD_PORT = os.environ['CDSW_READONLY_PORT']\n",
    "DASHBOARD_IP = os.environ['CDSW_IP_ADDRESS']\n",
    "\n",
    "ray_head_addr = DASHBOARD_IP + ':6379'\n",
    "ray_head_addr\n",
    "\n",
    "ray_url = f\"ray://{DASHBOARD_IP}:10001\" \n",
    "ray.init(ray_url)\n",
    "\n",
    "num_workers=3 #spawn new worker pod apart from this running pod\n",
    "\n",
    "worker_start_cmd = f\"!ray start --block --address={ray_head_addr}\"\n",
    "    \n",
    "ray_workers = cdsw.launch_workers(\n",
    "     n=num_workers, \n",
    "     cpu=4, \n",
    "     memory=16,\n",
    "     nvidia_gpu=1,\n",
    "     code=worker_start_cmd,\n",
    "     )\n",
    "\n",
    "ray_worker_details = cdsw.await_workers(\n",
    "     ray_workers, \n",
    "     wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d7f0a5-f65d-4ca4-a475-9e745472edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 11:57:51 llm_engine.py:70] Initializing an LLM engine with config: model='vicuna-13b-v1.3', tokenizer='vicuna-13b-v1.3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 11:58:12 llm_engine.py:275] # GPU blocks: 182, # CPU blocks: 327\n",
      "INFO 01-23 11:58:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 11:58:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "INFO 01-23 11:58:20 model_runner.py:547] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:02<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='Tell me 3 benefits of K8s', prompt_token_ids=[1, 24948, 592, 29871, 29941, 23633, 310, 476, 29947, 29879], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' and 3 drawbacks.\\n\\nBenefits:\\n\\n1. Scalability: Kubernetes is designed to handle large-scale deployments and can easily scale up or down as needed.\\n2. Flexibility: Kubernetes provides a flexible platform for deploying and managing containerized applications, allowing for easy updates and changes to the application environment.\\n3. Resilience: Kubernetes provides built-in resilience features, such as automatic failover and self-healing capabilities, to ensure that applications continue to run even in the face of hardware or software failures.\\n\\nDrawback', token_ids=[322, 29871, 29941, 4216, 1627, 29879, 29889, 13, 13, 20841, 1389, 1169, 29901, 13, 13, 29896, 29889, 317, 1052, 3097, 29901, 476, 17547, 338, 8688, 304, 4386, 2919, 29899, 7052, 7246, 1860, 322, 508, 5948, 6287, 701, 470, 1623, 408, 4312, 29889, 13, 29906, 29889, 383, 2506, 4127, 29901, 476, 17547, 8128, 263, 25706, 7481, 363, 7246, 292, 322, 767, 6751, 5639, 1891, 8324, 29892, 14372, 363, 4780, 11217, 322, 3620, 304, 278, 2280, 5177, 29889, 13, 29941, 29889, 2538, 2638, 663, 29901, 476, 17547, 8128, 4240, 29899, 262, 620, 2638, 663, 5680, 29892, 1316, 408, 18428, 4418, 957, 322, 1583, 29899, 354, 12818, 27108, 29892, 304, 9801, 393, 8324, 6773, 304, 1065, 1584, 297, 278, 3700, 310, 12837, 470, 7047, 4418, 1973, 29889, 13, 13, 8537, 1627], cumulative_logprob=-38.12197102910841, logprobs=None, finish_reason=length)], finished=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\"vicuna-13b-v1.3\", gpu_memory_utilization=0.7, tensor_parallel_size=1)\n",
    "sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=128)\n",
    "output = llm.generate(\"Tell me 3 benefits of K8s\",sampling_params)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416e694a-e1ad-453b-b934-3dc94efcbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496380fa-3d24-4143-af95-635bb3d08b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 11:59:53,619\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.254.21.246:6379...\n",
      "2024-01-23 11:59:53,792\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8100 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 11:59:54 llm_engine.py:70] Initializing an LLM engine with config: model='vicuna-13b-v1.3', tokenizer='vicuna-13b-v1.3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 12:00:43 llm_engine.py:275] # GPU blocks: 2339, # CPU blocks: 655\n",
      "INFO 01-23 12:00:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 12:00:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "\u001b[36m(RayWorkerVllm pid=581, ip=10.254.19.59)\u001b[0m INFO 01-23 12:00:45 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=581, ip=10.254.19.59)\u001b[0m INFO 01-23 12:00:45 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerVllm pid=581, ip=10.254.19.59)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 12:01:20 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                  | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=581, ip=10.254.19.59)\u001b[0m INFO 01-23 12:01:20 model_runner.py:547] Graph capturing finished in 35 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:04<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='Tell me 3 benefits of K8s', prompt_token_ids=[1, 24948, 592, 29871, 29941, 23633, 310, 476, 29947, 29879], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' and 3 drawbacks.\\n\\nBenefits:\\n\\n1. Scalability: Kubernetes is designed to handle large-scale deployments and can easily scale up or down as needed.\\n2. Flexibility: Kubernetes provides a flexible platform for deploying and managing containerized applications, allowing for easy updates and changes to the application environment.\\n3. Resilience: Kubernetes provides built-in resilience features, such as automatic failover and self-healing capabilities, to ensure that applications continue to run even in the face of hardware or software failures.\\n\\nDrawback', token_ids=[322, 29871, 29941, 4216, 1627, 29879, 29889, 13, 13, 20841, 1389, 1169, 29901, 13, 13, 29896, 29889, 317, 1052, 3097, 29901, 476, 17547, 338, 8688, 304, 4386, 2919, 29899, 7052, 7246, 1860, 322, 508, 5948, 6287, 701, 470, 1623, 408, 4312, 29889, 13, 29906, 29889, 383, 2506, 4127, 29901, 476, 17547, 8128, 263, 25706, 7481, 363, 7246, 292, 322, 767, 6751, 5639, 1891, 8324, 29892, 14372, 363, 4780, 11217, 322, 3620, 304, 278, 2280, 5177, 29889, 13, 29941, 29889, 2538, 2638, 663, 29901, 476, 17547, 8128, 4240, 29899, 262, 620, 2638, 663, 5680, 29892, 1316, 408, 18428, 4418, 957, 322, 1583, 29899, 354, 12818, 27108, 29892, 304, 9801, 393, 8324, 6773, 304, 1065, 1584, 297, 278, 3700, 310, 12837, 470, 7047, 4418, 1973, 29889, 13, 13, 8537, 1627], cumulative_logprob=-38.12242880134772, logprobs=None, finish_reason=length)], finished=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\"vicuna-13b-v1.3\", gpu_memory_utilization=0.7, tensor_parallel_size=2)\n",
    "sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=128)\n",
    "output = llm.generate(\"Tell me 3 benefits of K8s\",sampling_params)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67ad224-eb51-44f7-a5b1-2ae9b2007712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a75be5-9009-44f0-b436-3febe0554751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 12:02:34,260\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.254.21.246:6379...\n",
      "2024-01-23 12:02:34,297\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8100 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 12:02:34 llm_engine.py:70] Initializing an LLM engine with config: model='vicuna-13b-v1.3', tokenizer='vicuna-13b-v1.3', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=4, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 12:03:31 llm_engine.py:275] # GPU blocks: 6687, # CPU blocks: 1310\n",
      "INFO 01-23 12:03:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-23 12:03:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n",
      "\u001b[36m(RayWorkerVllm pid=658, ip=10.254.19.59)\u001b[0m INFO 01-23 12:03:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=658, ip=10.254.19.59)\u001b[0m INFO 01-23 12:03:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=658, ip=10.254.19.59)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 12:04:08 model_runner.py:547] Graph capturing finished in 34 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                  | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=506, ip=10.254.20.83)\u001b[0m INFO 01-23 12:04:08 model_runner.py:547] Graph capturing finished in 34 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=502, ip=10.254.18.76)\u001b[0m INFO 01-23 12:03:34 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=502, ip=10.254.18.76)\u001b[0m INFO 01-23 12:03:34 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:07<00:00,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='Tell me 3 benefits of K8s', prompt_token_ids=[1, 24948, 592, 29871, 29941, 23633, 310, 476, 29947, 29879], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' and 3 drawbacks.\\n\\nBenefits:\\n\\n1. Scalability: Kubernetes is designed to handle large-scale deployments and can easily scale up or down as needed.\\n2. Flexibility: Kubernetes provides a flexible platform for deploying and managing containerized applications, allowing for easy updates and changes to the application environment.\\n3. Resilience: Kubernetes provides built-in resilience features, such as automatic failover and self-healing capabilities, to ensure that applications continue to run even in the face of hardware or software failures.\\n\\nDrawback', token_ids=[322, 29871, 29941, 4216, 1627, 29879, 29889, 13, 13, 20841, 1389, 1169, 29901, 13, 13, 29896, 29889, 317, 1052, 3097, 29901, 476, 17547, 338, 8688, 304, 4386, 2919, 29899, 7052, 7246, 1860, 322, 508, 5948, 6287, 701, 470, 1623, 408, 4312, 29889, 13, 29906, 29889, 383, 2506, 4127, 29901, 476, 17547, 8128, 263, 25706, 7481, 363, 7246, 292, 322, 767, 6751, 5639, 1891, 8324, 29892, 14372, 363, 4780, 11217, 322, 3620, 304, 278, 2280, 5177, 29889, 13, 29941, 29889, 2538, 2638, 663, 29901, 476, 17547, 8128, 4240, 29899, 262, 620, 2638, 663, 5680, 29892, 1316, 408, 18428, 4418, 957, 322, 1583, 29899, 354, 12818, 27108, 29892, 304, 9801, 393, 8324, 6773, 304, 1065, 1584, 297, 278, 3700, 310, 12837, 470, 7047, 4418, 1973, 29889, 13, 13, 8537, 1627], cumulative_logprob=-38.09864003552593, logprobs=None, finish_reason=length)], finished=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\"vicuna-13b-v1.3\", gpu_memory_utilization=0.7, tensor_parallel_size=4)\n",
    "sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=128)\n",
    "output = llm.generate(\"Tell me 3 benefits of K8s\",sampling_params)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a8564-609a-4d63-8b24-7c4316f792f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark_throughput.py --backend vllm --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json --model open_llama_13b  --num-prompts=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e98cf-3646-4583-a028-39c6ee18aa22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python benchmark_throughput.py --backend vllm --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json --model open_llama_13b  --num-prompts=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db761d9d-7961-4751-a0e9-55fa6c8721ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark_throughput.py --backend vllm --tensor-parallel-size 2 --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json --model open_llama_13b  --num-prompts=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4d719-1e66-4d7e-b635-2f5d0e9c4146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python benchmark_throughput.py --backend vllm --tensor-parallel-size 4 --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json --model vicuna-13b-v1.3  --num-prompts=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6134a5-e819-4ff4-a6c7-06856aedba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark_latency.py --tensor-parallel-size 1 --model vicuna-13b-v1.3  --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a39a93-edf6-49f9-9e8c-2434b772d23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python benchmark_latency.py --tensor-parallel-size 2 --model vicuna-13b-v1.3  --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4c87e-80f9-4558-8af2-28e01ffe4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark_latency.py --tensor-parallel-size 4 --model vicuna-13b-v1.3  --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c4538-2aa4-40bd-b1a8-af9cf2bb7aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
